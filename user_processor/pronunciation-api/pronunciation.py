from fastapi import FastAPI, Form, BackgroundTasks
from fastapi.responses import JSONResponse
from datetime import datetime
from zoneinfo import ZoneInfo
import time
from urllib.parse import urlparse
import uuid
import boto3
import tempfile
import json
import numpy as np
import shutil
import requests
import re
from pathlib import Path
from run_whisper_cpp import speech_to_text
from text_similarity import compare_texts, parse_whisper_cpp_result, normalize_segments_to_zero, parse_time, normalize_and_tokenize
from sklearn.metrics.pairwise import cosine_similarity
from mfcc_similarity import extract_mfcc_from_audio, extract_mfcc_segment, compare_mfcc_segments

import threading
import asyncio

app = FastAPI(
    title = "Voice Analysis API",
    description = "ìœ ì € ìŒì„± ë¶„ì„ ë° í‰ê°€ ìë™í™” API")

# SQS ì„¤ì • ì¶”ê°€
SQS_QUEUE_URL = 'https://sqs.ap-northeast-2.amazonaws.com/975049946580/audio-analysis-queue'
sqs_client = boto3.client('sqs', region_name='ap-northeast-2')

# ì‘ì—… ìƒíƒœ ì €ì¥ ë”•ì…”ë„ˆë¦¬
job_status = {}

@app.get("/")
async def root():
    return {"message" : "Voice Analysis API", "status" : "running"}

@app.get("/status")
async def server_status():
    return {
        "status" : "running",
        "timestamp" : datetime.now().isoformat(),
        "message" : "Voice Analysis API ì •ìƒ ì‘ë™ ì¤‘"
    }

@app.post("/analyze-voice")
async def analyze_voice(
    background_tasks: BackgroundTasks,
    request_data: str = Form(...)
):
    # ê³ ìœ í•œ job_id ìƒì„±
    job_id = str(uuid.uuid4())

    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    start_datetime_kst = datetime.now(ZoneInfo("Asia/Seoul"))
    print(f"[{job_id}] ğŸš€ ìš”ì²­ ì‹œì‘: {start_datetime_kst.strftime('%Y-%m-%d %H:%M:%S KST')}")
    
    data = json.loads(request_data)
    s3_audio_url = data["s3_audio_url"]
    webhook_url = data["webhook_url"]
    script_data = data["script"]

    # ê¸°ë³¸ ì •ë³´ ë¡œê·¸
    print(f"[{job_id}] ğŸ“ S3 URL: {s3_audio_url}")

    try:
        # ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
        job_status[job_id] = {
            "status": "processing",
            "s3_audio_url": s3_audio_url,
            "started_at": datetime.now().isoformat()
        }

        audio_path = download_from_s3(s3_audio_url, job_id)

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
        background_tasks.add_task(analyze_pronunciation_pipeline, job_id, audio_path, script_data, s3_audio_url, webhook_url, start_time)

        return {
            "job_id": job_id,
            "status": "processing",
            "message": "ìŒì„± ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except ValueError as e:
        job_status[job_id] = {
            "status": "failed",
            "s3_audio_url": s3_audio_url,
            "started_at": datetime.now().isoformat(),
            "error": str(e)
        }
        return JSONResponse(
            status_code=400,
            content={"error": f"ì˜ëª»ëœ ìš”ì²­: {str(e)}"}
        )

    except Exception as e:
        job_status[job_id] = {
            "status": "failed",
            "s3_audio_url": s3_audio_url,
            "started_at": datetime.now().isoformat(),
            "error": str(e)
        }
        return JSONResponse(
            status_code=500,
            content={"error": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"}
        )

def parse_s3_url(s3_url: str) -> tuple[str, str]:
    if s3_url.startswith('s3://'):
        parts = s3_url.split('/', 3)
        if len(parts) < 4:
            raise ValueError("ì˜¬ë°”ë¥¸ s3://bucket/key í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return parts[2], parts[3]
    elif 'amazonaws.com' in s3_url:
        parsed = urlparse(s3_url)
        netloc_parts = parsed.netloc.split('.')
        if len(netloc_parts) < 3 or not parsed.path:
            raise ValueError("ì˜¬ë°”ë¥´ì§€ ì•Šì€ S3 URL í˜•ì‹ì…ë‹ˆë‹¤.")
        bucket = netloc_parts[0]
        key = parsed.path.lstrip('/')
        return bucket, key
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•Šì€ S3 URL í˜•ì‹ì…ë‹ˆë‹¤.")

def download_from_s3(s3_audio_url: str, job_id: str) -> str:
    s3_client = boto3.client('s3')
    audio_bucket, audio_key = parse_s3_url(s3_audio_url)

    job_folder = Path(f"/tmp/{job_id}")
    job_folder.mkdir(exist_ok=True)

    audio_path = job_folder / "user_audio.wav"
    s3_client.download_file(audio_bucket, audio_key, str(audio_path))

    # shared_data í´ë”ì—ë„ ë°±ì—… ì €ì¥
    import os
    shared_dir = os.path.expanduser('~/shared_data/user')
    os.makedirs(shared_dir, exist_ok=True)
    shared_file_path = f"{shared_dir}/{job_id}.wav"
    shutil.copy2(str(audio_path), shared_file_path)
    return str(audio_path)
    print(f"[{job_id}] ğŸ“ ìœ ì € ìŒì„± ë°±ì—…: {shared_file_path}")

def calculate_time_overlap(ref_start, ref_end, user_start, user_end):
    """ë‘ ì‹œê°„ êµ¬ê°„ì˜ ê²¹ì¹¨ ì •ë„ë¥¼ ê³„ì‚°"""
    overlap_start = max(ref_start, user_start)
    overlap_end = min(ref_end, user_end)
    
    if overlap_start >= overlap_end:
        return 0.0, 0.0  # ê²¹ì¹¨ ì—†ìŒ
    
    overlap_duration = overlap_end - overlap_start
    ref_duration = ref_end - ref_start
    overlap_ratio = overlap_duration / ref_duration if ref_duration > 0 else 0.0
    
    return overlap_duration, overlap_ratio

def log_time_overlap_analysis(job_id, ref_word, ref_start, ref_end, user_word, user_start, user_end, overlap_ratio, threshold=0.4):
    """ì‹œê°„ ê²¹ì¹¨ ë¶„ì„ ë¡œê·¸ ì¶œë ¥"""
    ref_duration = ref_end - ref_start
    user_duration = user_end - user_start
    start_diff = user_start - ref_start
    end_diff = user_end - ref_end
    
    if overlap_ratio >= threshold:
        status = "âœ…"
        result = "ë§¤ì¹­"
    else:
        status = "âŒ" if overlap_ratio < threshold else "âš ï¸"
        result = "ë§¤ì¹­ ì‹¤íŒ¨" if overlap_ratio < threshold else "ë§¤ì¹­ (ì‹œê°„ ë¶ˆì¼ì¹˜)"
    
    print(f"[{job_id}]   {status} \"{ref_word}\" {result}:")
    print(f"[{job_id}]     â””â”€ ê¸°ì¤€: {ref_start:.3f}s~{ref_end:.3f}s vs ìœ ì €: {user_start:.3f}s~{user_end:.3f}s")
    print(f"[{job_id}]     â””â”€ ê²¹ì¹¨: {max(ref_start, user_start):.3f}s~{min(ref_end, user_end):.3f}s ({overlap_ratio*ref_duration:.3f}s) / ê¸°ì¤€ê¸¸ì´: {ref_duration:.3f}s = {overlap_ratio*100:.1f}% ê²¹ì¹¨")
    print(f"[{job_id}]     â””â”€ ì‹œì‘ì  ì°¨ì´: {start_diff:+.3f}s, ì¢…ë£Œì  ì°¨ì´: {end_diff:+.3f}s")
    
    if overlap_ratio < threshold:
        print(f"[{job_id}]     â””â”€ ì‹¤íŒ¨ ì›ì¸: ê²¹ì¹¨ë¥  {overlap_ratio*100:.1f}% < ì„ê³„ê°’ {threshold*100:.0f}%")
    
    return overlap_ratio >= threshold

def extract_word_timestamps(stt_result):
    """STT ê²°ê³¼ì—ì„œ ë‹¨ì–´ë³„ ì‹œê°„ ì •ë³´ ì¶”ì¶œ"""
    word_timestamps = []
    
    if not stt_result.get('transcription'):
        return word_timestamps
    
    transcription = stt_result['transcription'][0]
    
    if 'words' in transcription and isinstance(transcription['words'], list) and transcription['words']:
        word_timestamps = []
        for word in transcription['words']:
            if 'word' in word and 'start' in word and 'end' in word:
                word_timestamps.append({
                    'word': word['word'],
                    'start_time': word['start'],
                    'end_time': word['end']
                    })
        return word_timestamps

    # tokens ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    tokens = transcription.get('tokens', [])

    # tokensê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not tokens:
        return word_timestamps
    
    # ë‹¨ì–´ ë³‘í•©ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
    current_text = ""       # ë³‘í•©ì¤‘ì¸ ë‹¨ì–´ í…ìŠ¤íŠ¸
    word_start = None       # í˜„ì¬ ë‹¨ì–´ì˜ ì‹œì‘ ì‹œê°„
    prev_end = None         # ì§ì „ í† í°ì˜ ì¢…ë£Œ ì‹œê°„

    # ê° í† í°ì„ ìˆœíšŒí•˜ë©° ë‹¨ì–´ ë³‘í•© ë° íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
    for token in tokens:
        # ì œì–´ í† í° ìŠ¤í‚µ
        text = token.get('text', '')
        if text.startswith('[') and text.endswith(']'):
            continue
        # ìˆœìˆ˜ êµ¬ë‘ì  í† í° ì œì™¸
        if re.fullmatch(r'[\W_]+', text):
            continue
        # í† í°ì˜ ì‹œì‘, ì¢…ë£Œ ì‹œê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ íŒŒì‹±
        start = parse_time(token['timestamps']['from'])
        end = parse_time(token['timestamps']['to'])

        # subword ê²½ê³„: ì•ì— ê³µë°±ì´ ìˆìœ¼ë©´ ìƒˆ ë‹¨ì–´ ì‹œì‘
        if text.startswith(' '):
            # ì´ì „ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ë¨¼ì € ì €ì¥
            if current_text:
                word_timestamps.append({
                    'word': current_text,
                    'start_time': word_start,
                    'end_time': prev_end
                })
            current_text = text.strip()
            word_start = start
        else:
            # subword ì´ì–´ ë¶™ì´ê¸°
            current_text += text.strip()

        #prev_end ê°±ì‹ 
        prev_end = end

    # ë§ˆì§€ë§‰ ë‹¨ì–´ ì €ì¥
    if current_text:
        word_timestamps.append({
            'word': current_text,
            'start_time': word_start,
            'end_time': prev_end
        })
    return word_timestamps

async def analyze_pronunciation_pipeline(job_id: str, audio_path: str, script_data: dict, s3_audio_url: str, webhook_url: str, start_time: float):
    """
    ì‹œê°„ ì •ë³´ì™€ ê²¹ì¹¨ ë¶„ì„ì´ í¬í•¨ëœ ê°œì„ ëœ ë°œìŒ ë¶„ì„ íŒŒì´í”„ë¼ì¸
    """
    
    step_times = {}  # ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
    
    try:
        # ìœ ì € ìŒì„± íŒŒì¼ ê¸¸ì´ í™•ì¸
        try:
            import librosa
            audio_duration = librosa.get_duration(path=str(audio_path))
            print(f"[{job_id}] ğŸ“Š ìœ ì € ìŒì„± ê¸¸ì´: {audio_duration:.3f}ì´ˆ")
        except Exception as e:
            print(f"[{job_id}] âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨: {e}")
            audio_duration = 0.0

        # ê¸°ì¤€ ìŠ¤í¬ë¦½íŠ¸ ì •ë³´ ë¡œê·¸
        reference_words = [word_data['word'] for word_data in script_data['words']]
        reference_text = ' '.join(reference_words)
        
        # ê¸°ì¤€ ìŠ¤í¬ë¦½íŠ¸ ì´ ê¸¸ì´ ê³„ì‚°
        if script_data['words'] and script_data['words'][-1].get('end_time', None) is not None:
            first_start = script_data['words'][0].get('start_time', 0)
            last_end = script_data['words'][-1]['end_time']
            ref_total_duration = last_end - first_start  # ìƒëŒ€ì  ê¸¸ì´ ê³„ì‚°
        
        print(f"[{job_id}] ğŸ“ ê¸°ì¤€ ë¬¸ì¥: \"{reference_text}\"")
        print(f"[{job_id}] ğŸ¯ ê¸°ì¤€ ë‹¨ì–´ ìˆ˜: {len(reference_words)}ê°œ, ì´ ê¸¸ì´: {ref_total_duration:.3f}ì´ˆ")

        # Step 1: STT ë³€í™˜
        stt_start = time.time()
        print(f"[{job_id}] ğŸ¤ STT ë³€í™˜ ì¤‘...")
        
        output_dir = Path(audio_path).parent
        model_path = Path("./whisper.cpp/models/ggml-medium.en.bin")
        
        user_stt_result = speech_to_text(audio_path=Path(audio_path), output_dir=output_dir, model_path=model_path)
        
        stt_end = time.time()
        step_times['stt'] = stt_end - stt_start
        
        # STT ê²°ê³¼ ì¶”ì¶œ
        user_text = ""
        if user_stt_result.get('transcription'):
            user_text = user_stt_result['transcription'][0].get('text', '').strip()
        
        print(f"[{job_id}] ğŸ¤ STT ì¸ì‹ ê²°ê³¼: \"{user_text}\"")
        # ìœ ì € ë‹¨ì–´ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
        reference_segments = normalize_segments_to_zero(script_data['words'])
        if user_text.startswith('['):
            print(f"[{job_id}] âš ï¸ ìŒì„± ì‹ í˜¸ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ ({user_text})")
            user_word_timestamps = []
        else:
            # STT í† í°ì—ì„œ ì§ì ‘ word-level íƒ€ì„ ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            raw_timestamps = extract_word_timestamps(user_stt_result)
            # 0ì´ˆ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            user_word_timestamps = raw_timestamps

        user_words = [item['word'] for item in user_word_timestamps]
        print(f"[{job_id}] ğŸ“Š ì¸ì‹ëœ ë‹¨ì–´ ìˆ˜: {len(user_words)}ê°œ (ê¸°ì¤€: {len(reference_words)}ê°œ)")
        if len(user_words) != len(reference_words):
            diff = len(reference_words) - len(user_words)
            if diff > 0:
                print(f"[{job_id}] âš ï¸ ë‹¨ì–´ ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€: {diff}ê°œ ë‹¨ì–´ ëˆ„ë½")
            else:
                print(f"[{job_id}] âš ï¸ ë‹¨ì–´ ìˆ˜ ë¶ˆì¼ì¹˜ ê°ì§€: {abs(diff)}ê°œ ë‹¨ì–´ ì´ˆê³¼")

        # ìœ ì € ë‹¨ì–´ë³„ ì‹œê°„ ì •ë³´ ë¡œê·¸
        print(f"[{job_id}] â° ìœ ì € ë‹¨ì–´ë³„ ì‹œê°„ ì •ë³´:")
        for item in user_word_timestamps:
            start = item.get('start_time')
            end = item.get('end_time')
            # Noneì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if start is None or end is None:
                continue
            duration = end - start
            print(f"[{job_id}]   \"{item['word']}\": {start:.3f}s ~ {end:.3f}s (ê¸¸ì´: {duration:.3f}s)")

        # ê¸°ì¤€ ë‹¨ì–´ë³„ ì‹œê°„ ì •ë³´ ë¡œê·¸
        print(f"[{job_id}] ğŸ“‹ ê¸°ì¤€ ë‹¨ì–´ë³„ ì‹œê°„ ì •ë³´:")
        for word_data in reference_segments:
            word = word_data['word']
            start = word_data.get('start_time', 0.0)
            end = word_data.get('end_time', 0.0)
            duration = end - start
            print(f"[{job_id}]   \"{word}\": {start:.3f}s ~ {end:.3f}s (ê¸¸ì´: {duration:.3f}s)")

        # Step 2: ì‹œê°„ ê²¹ì¹¨ ë¶„ì„
        overlap_start = time.time()
        
        time_matching_results = analyze_time_overlap(job_id, reference_segments, user_word_timestamps)
        
        overlap_end = time.time()
        step_times['overlap'] = overlap_end - overlap_start

        # Step 3: í…ìŠ¤íŠ¸ ë¹„êµ
        text_start = time.time()
        
        text_comparison_result = compare_texts(reference_segments, user_stt_result)
        
        text_end = time.time()
        step_times['text'] = text_end - text_start
        
        # í…ìŠ¤íŠ¸ ë§¤ì¹­ ìš”ì•½ ë¡œê·¸
        log_text_matching_summary(job_id, text_comparison_result, time_matching_results, [word_data['word'] for word_data in reference_segments])

        # Step 4: MFCC ë¶„ì„
        mfcc_start = time.time()
        
        user_mfcc, user_frame_times = extract_mfcc_from_audio(audio_path)
        mfcc_comparison_result = compare_mfcc_segments(
            cached_segments=reference_segments, 
            user_mfcc=user_mfcc, 
            user_frame_times=user_frame_times, 
            job_id=job_id
        )
        
        mfcc_end = time.time()
        step_times['mfcc'] = mfcc_end - mfcc_start
        
        # MFCC ê²°ê³¼ ìƒì„¸ ë¡œê·¸
        log_mfcc_analysis_with_time(job_id, reference_segments, mfcc_comparison_result, time_matching_results, user_word_timestamps)

        # Step 5: ì¢…í•© ê²°ê³¼ ìƒì„±
        final_results = generate_comprehensive_results(
            job_id, [word_data['word'] for word_data in reference_segments], text_comparison_result, 
            mfcc_comparison_result, time_matching_results
        )
        
        # ìœ ì € STT ê²°ê³¼ ì¶”ê°€
        user_stt_data = {
            "text": user_text,
            "word_timestamps": user_word_timestamps
        }
        final_results["user_stt"] = user_stt_data
        
        # Step 6: ì›¹í›… ì „ì†¡
        webhook_start = time.time()
        print(f"[{job_id}] ğŸ“¤ ì›¹í›… ì „ì†¡ ì¤‘...")
        
        webhook_response = requests.post(webhook_url, json={
            "job_id": job_id,
            "status": "completed",
            "result": final_results
        }, timeout=10)
        
        webhook_end = time.time()
        step_times['webhook'] = webhook_end - webhook_start
        
        total_end = time.time()
        total_duration = total_end - start_time
        
        print(f"[{job_id}] â±ï¸ ì²˜ë¦¬ ì‹œê°„ ë¶„ì„:")
        print(f"[{job_id}]   STT ë³€í™˜: {step_times['stt']:.2f}ì´ˆ")
        print(f"[{job_id}]   ì‹œê°„ ê²¹ì¹¨ ë¶„ì„: {step_times['overlap']:.2f}ì´ˆ")
        print(f"[{job_id}]   í…ìŠ¤íŠ¸ ë¹„êµ: {step_times['text']:.2f}ì´ˆ")
        print(f"[{job_id}]   MFCC ë¶„ì„: {step_times['mfcc']:.2f}ì´ˆ")
        print(f"[{job_id}]   ì›¹í›… ì „ì†¡: {step_times['webhook']:.2f}ì´ˆ")
        print(f"[{job_id}] ğŸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_duration:.2f}ì´ˆ (ìƒíƒœ: {webhook_response.status_code})")

    except Exception as e:
        print(f"[{job_id}] âŒ íŒŒì´í”„ë¼ì¸ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        
        # ì‹¤íŒ¨ ì›¹í›… ì „ì†¡
        try:
            webhook_start = time.time()
            requests.post(webhook_url, json={
                "job_id": job_id,
                "status": "failed",
                "error": str(e)
            }, timeout=10)
            webhook_end = time.time()
            total_end = time.time()
            print(f"[{job_id}] â±ï¸ ì‹¤íŒ¨ ì›¹í›… ì „ì†¡ ì‹œê°„: {webhook_end - webhook_start:.2f}ì´ˆ")
            print(f"[{job_id}] ğŸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - start_time:.2f}ì´ˆ")
        except Exception as webhook_error:
            print(f"[{job_id}] âŒ ì›¹í›… í˜¸ì¶œ ì‹¤íŒ¨: {webhook_error}")

    finally:
        try:
            job_folder = Path(audio_path).parent
            shutil.rmtree(job_folder)
            print(f"[{job_id}] ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        except Exception as cleanup_error:
            print(f"[{job_id}] âš ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {cleanup_error}")
            
def analyze_time_overlap(job_id, reference_words, user_word_timestamps, threshold=0.4):
    """ì‹œê°„ ê²¹ì¹¨ ë¶„ì„ ìˆ˜í–‰ ë° ìƒì„¸ ë¡œê·¸ ì¶œë ¥"""
    print(f"[{job_id}] ğŸ” ë‹¨ì–´ë³„ ì‹œê°„ ê²¹ì¹¨ ë¶„ì„:")
    
    time_matching_results = []
    
    # ìœ ì € ë‹¨ì–´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´)
    #user_words_dict = {item['word'].lower(): item for item in user_word_timestamps}
    user_words_dict = {
        normalize_and_tokenize(item['word'])[0]: item for item in user_word_timestamps
    }
    
    for ref_word_data in reference_words:
        ref_word = ref_word_data['word']
        ref_start = ref_word_data.get('start_time', 0.0)
        ref_end = ref_word_data.get('end_time', 0.0)
        
        # í•´ë‹¹ ë‹¨ì–´ê°€ ìœ ì € STTì— ìˆëŠ”ì§€ í™•ì¸
        ref_key = normalize_and_tokenize(ref_word)[0]
        user_match = user_words_dict.get(ref_key)
        
        if user_match:
            user_start = user_match['start_time']
            user_end = user_match['end_time']
            
            overlap_duration, overlap_ratio = calculate_time_overlap(ref_start, ref_end, user_start, user_end)
            is_time_match = log_time_overlap_analysis(job_id, ref_word, ref_start, ref_end, ref_word, user_start, user_end, overlap_ratio, threshold)
            
            time_matching_results.append({
                'word': ref_word,
                'time_match': is_time_match,
                'overlap_ratio': overlap_ratio,
                'ref_start': ref_start,
                'ref_end': ref_end,
                'user_start': user_start,
                'user_end': user_end
            })
        else:
            print(f"[{job_id}]   âŒ \"{ref_word}\" ë§¤ì¹­ ì‹¤íŒ¨:")
            print(f"[{job_id}]     â””â”€ ê¸°ì¤€: {ref_start:.3f}s~{ref_end:.3f}s vs ìœ ì €: ë§¤ì¹­ í›„ë³´ ì—†ìŒ")
            print(f"[{job_id}]     â””â”€ ì‹¤íŒ¨ ì›ì¸: STTì—ì„œ ì¸ì‹ë˜ì§€ ì•ŠìŒ")
            
            time_matching_results.append({
                'word': ref_word,
                'time_match': False,
                'overlap_ratio': 0.0,
                'ref_start': ref_start,
                'ref_end': ref_end,
                'user_start': None,
                'user_end': None
            })
    
    return time_matching_results

def log_text_matching_summary(job_id, text_comparison_result, time_matching_results, reference_words):
    """í…ìŠ¤íŠ¸ ë§¤ì¹­ ìš”ì•½ ë¡œê·¸ ì¶œë ¥"""
    print(f"[{job_id}] ğŸ“ í…ìŠ¤íŠ¸ ë§¤ì¹­ ìš”ì•½:")
    
    complete_success = 0  # í…ìŠ¤íŠ¸ + ì‹œê°„ ëª¨ë‘ ì„±ê³µ
    text_only_success = 0  # í…ìŠ¤íŠ¸ë§Œ ì„±ê³µ
    time_insufficient = 0  # ì‹œê°„ ê²¹ì¹¨ ë¶€ì¡±
    stt_failure = 0  # STT ì¸ì‹ ì‹¤íŒ¨
    
    for i, word in enumerate(reference_words):
        text_status = text_comparison_result[i]["status"] if i < len(text_comparison_result) else "fail"
        time_result = time_matching_results[i] if i < len(time_matching_results) else None
        
        if text_status == "pass" and time_result and time_result['time_match']:
            complete_success += 1
        elif text_status == "pass" and time_result and not time_result['time_match'] and time_result['user_start'] is not None:
            text_only_success += 1
        elif text_status == "fail" and time_result and time_result['user_start'] is not None:
            time_insufficient += 1
        else:
            stt_failure += 1
    
    print(f"[{job_id}]   âœ… ì™„ì „ ì„±ê³µ: {complete_success}ê°œ (í…ìŠ¤íŠ¸ + ì‹œê°„ ë§¤ì¹­)")
    if text_only_success > 0:
        print(f"[{job_id}]   âš ï¸ í…ìŠ¤íŠ¸ë§Œ ì¼ì¹˜: {text_only_success}ê°œ (ì‹œê°„ ë¶ˆì¼ì¹˜)")
    if time_insufficient > 0:
        print(f"[{job_id}]   âŒ ì‹œê°„ ê²¹ì¹¨ ë¶€ì¡±: {time_insufficient}ê°œ")
    if stt_failure > 0:
        print(f"[{job_id}]   âŒ STT ì¸ì‹ ì‹¤íŒ¨: {stt_failure}ê°œ")
    
    text_success_rate = (complete_success + text_only_success) / len(reference_words) * 100
    time_success_rate = complete_success / len(reference_words) * 100
    
    print(f"[{job_id}] ğŸ“ˆ í…ìŠ¤íŠ¸ ë§¤ì¹­ ì„±ê³µë¥ : {complete_success + text_only_success}/{len(reference_words)} ({text_success_rate:.1f}%)")
    print(f"[{job_id}] â° ì‹œê°„ ë§¤ì¹­ ì„±ê³µë¥ : {complete_success}/{len(reference_words)} ({time_success_rate:.1f}%)")

def log_mfcc_analysis_with_time(job_id, reference_words, mfcc_comparison_result, time_matching_results, user_word_timestamps):
    """MFCC ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°„ ì •ë³´ì™€ í•¨ê»˜ ë¡œê·¸ ì¶œë ¥"""
    print(f"[{job_id}] ğŸµ MFCC ìœ ì‚¬ë„ ë¶„ì„:")
    
    time_matched_scores = []
    all_scores = []
    
    for i, word_data in enumerate(reference_words):
        word = word_data['word']
        ref_start = word_data.get('start_time', 0.0)
        ref_end = word_data.get('end_time', 0.0)
        
        mfcc_similarity = 0.0
        if i < len(mfcc_comparison_result):
            mfcc_similarity = mfcc_comparison_result[i].get("similarity", 0.0)
        
        time_result = time_matching_results[i] if i < len(time_matching_results) else None
        
        if time_result and time_result['user_start'] is not None:
            user_start = time_result['user_start']
            user_end = time_result['user_end']
            
            if time_result['time_match']:
                status = "ğŸ¯"
                quality = get_mfcc_quality_description(mfcc_similarity)
                time_matched_scores.append(mfcc_similarity)
                print(f"[{job_id}]   {status} \"{word}\" (ì‹œê°„ë§¤ì¹­ ì„±ê³µ): {mfcc_similarity:.3f} ({quality})")
                print(f"[{job_id}]     â””â”€ ë¶„ì„êµ¬ê°„: ê¸°ì¤€ {ref_start:.3f}s~{ref_end:.3f}s vs ìœ ì € {user_start:.3f}s~{user_end:.3f}s")
            else:
                status = "âš ï¸"
                quality = get_mfcc_quality_description(mfcc_similarity)
                print(f"[{job_id}]   {status} \"{word}\" (ì‹œê°„ë§¤ì¹­ ì‹¤íŒ¨): {mfcc_similarity:.3f} ({quality})")
                print(f"[{job_id}]     â””â”€ ë¶„ì„êµ¬ê°„: ê¸°ì¤€ {ref_start:.3f}s~{ref_end:.3f}s vs ìœ ì € {user_start:.3f}s~{user_end:.3f}s")
                print(f"[{job_id}]     â””â”€ ë‚®ì€ ì´ìœ : ì‹œê°„ ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ ì˜ëª»ëœ êµ¬ê°„ ë¹„êµ")
            
            all_scores.append(mfcc_similarity)
        else:
            print(f"[{job_id}]   âŒ \"{word}\": N/A (STT ì¸ì‹ ì‹¤íŒ¨)")
    
    # í‰ê·  ê³„ì‚°
    if all_scores:
        overall_avg = sum(all_scores) / len(all_scores)
        print(f"[{job_id}] ğŸ“Š í‰ê·  MFCC ìœ ì‚¬ë„: {overall_avg:.3f}", end="")
        
        if time_matched_scores:
            time_matched_avg = sum(time_matched_scores) / len(time_matched_scores)
            print(f" (ì‹œê°„ë§¤ì¹­ ì„±ê³µí•œ ë‹¨ì–´ë§Œ: {time_matched_avg:.3f})")
        else:
            print()
    else:
        print(f"[{job_id}] ğŸ“Š í‰ê·  MFCC ìœ ì‚¬ë„: N/A (ë¶„ì„ ê°€ëŠ¥í•œ ë‹¨ì–´ ì—†ìŒ)")

def get_mfcc_quality_description(similarity):
    """MFCC ìœ ì‚¬ë„ ì ìˆ˜ì— ë”°ë¥¸ í’ˆì§ˆ ì„¤ëª…"""
    if similarity >= 0.8:
        return "ìš°ìˆ˜"
    elif similarity >= 0.6:
        return "ì–‘í˜¸"
    elif similarity >= 0.4:
        return "ë³´í†µ"
    elif similarity >= 0.2:
        return "ë‚®ìŒ"
    else:
        return "ë§¤ìš° ë‚®ìŒ"

def generate_comprehensive_results(job_id, reference_words, text_comparison_result, mfcc_comparison_result, time_matching_results):
    """ì¢…í•© ê²°ê³¼ ìƒì„± ë° ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
    
    # ë‹¨ì–´ë³„ í†µí•© ê²°ê³¼ ìƒì„±
    word_analysis = []
    text_pass_count = 0
    time_pass_count = 0
    
    # ì‹¤íŒ¨ ì›ì¸ ë¶„ë¥˜
    stt_failures = []
    time_failures = []
    mfcc_low_quality = []
    
    for i, word in enumerate(reference_words):
        # í…ìŠ¤íŠ¸ ê²°ê³¼
        text_status = text_comparison_result[i]["status"] if i < len(text_comparison_result) else "fail"
        
        # ì‹œê°„ ë§¤ì¹­ ê²°ê³¼
        time_result = time_matching_results[i] if i < len(time_matching_results) else None
        time_match = time_result['time_match'] if time_result else False
        overlap_ratio = time_result['overlap_ratio'] if time_result else 0.0
        
        # MFCC ê²°ê³¼
        mfcc_similarity = 0.0
        if i < len(mfcc_comparison_result):
            # í™˜ì‚°ëœ ì ìˆ˜ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë˜ ìœ ì‚¬ë„ ì‚¬ìš©
            if "adjusted_score" in mfcc_comparison_result[i]:
                mfcc_similarity = mfcc_comparison_result[i].get("adjusted_score", 0.0)
            else:
                mfcc_similarity = mfcc_comparison_result[i].get("similarity", 0.0)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ 70% + MFCC 30%)
        confidence = text_comparison_result[i].get("confidence", 0.0)
        text_score = confidence if text_status == "pass" else 0.0
        #text_score = 1.0 if text_status == "pass" else 0.0
        print(f"âŒ{confidence * 0.7}")
        word_score = (text_score * 0.7) + (mfcc_similarity * 0.3)
        
        word_analysis.append({
            "word": word,
            "text_status": text_status,
            "time_match": time_match,
            "overlap_ratio": round(overlap_ratio, 3),
            "mfcc_similarity": round(mfcc_similarity, 3),
            "word_score": round(word_score, 3)
        })
        
        # í†µê³¼ ê°œìˆ˜ ê³„ì‚°
        if text_status == "pass":
            text_pass_count += 1
        if time_match:
            time_pass_count += 1
        
        # ì‹¤íŒ¨ ì›ì¸ ë¶„ë¥˜
        if time_result and time_result['user_start'] is None:
            stt_failures.append(word)
        elif not time_match and time_result and time_result['user_start'] is not None:
            time_failures.append({
                'word': word,
                'overlap_ratio': overlap_ratio
            })
        
        if mfcc_similarity < 0.4:  # ë‚®ì€ MFCC í’ˆì§ˆ
            mfcc_low_quality.append({
                'word': word,
                'similarity': mfcc_similarity
            })
    
    # ì „ì²´ ìš”ì•½ ê³„ì‚°
    text_accuracy = text_pass_count / len(reference_words) if reference_words else 0.0
    time_accuracy = time_pass_count / len(reference_words) if reference_words else 0.0
    
    # MFCC í‰ê·  ê³„ì‚°
    mfcc_total = sum(result["mfcc_similarity"] for result in word_analysis)
    mfcc_average = mfcc_total / len(word_analysis) if word_analysis else 0.0
    
    # ì „ì²´ ì ìˆ˜ ê³„ì‚°
    score_total = sum(result["word_score"] for result in word_analysis)
    overall_score = score_total / len(word_analysis) if word_analysis else 0.0
    
    # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ë¡œê·¸
    print(f"[{job_id}] ğŸ” ì‹¤íŒ¨ ì›ì¸ ìƒì„¸ ë¶„ì„:")
    
    if stt_failures:
        quoted_words = [f'"{w}"' for w in stt_failures]
        print(f"[{job_id}]   ğŸ“ STT ì¸ì‹ ì‹¤íŒ¨: {len(stt_failures)}ê°œ ë‹¨ì–´ ({', '.join(quoted_words)})")

        # If every word failed, assume blank audio
        if len(stt_failures) == len(reference_words):
            print(f"[{job_id}]     â””â”€ ì›ì¸: ìŒì„± ì‹ í˜¸ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ (ëª¨ë‘ ì¸ì‹ ì‹¤íŒ¨)")
            print(f"[{job_id}]     â””â”€ ì œì•ˆ: ë‹¤ì‹œ ë…¹ìŒí•˜ê±°ë‚˜ ë§ˆì´í¬ ë° ë³¼ë¥¨ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”")
        else:
            # Otherwise, check for short words
            short_words = [w for w in stt_failures if len(w) <= 2]
            if short_words:
                print(f"[{job_id}]     â””â”€ ì›ì¸: ì§§ì€ ë‹¨ì–´ ì¸ì‹ ì‹¤íŒ¨")
                print(f"[{job_id}]     â””â”€ ì œì•ˆ: ì§§ì€ ë‹¨ì–´ë¥¼ ë” ëª…í™•í•˜ê²Œ ë°œìŒ")
            else:
                print(f"[{job_id}]     â””â”€ ì œì•ˆ: ë°œìŒì„ ë” ëª…í™•í•˜ê²Œ í•˜ê±°ë‚˜ ì†ë„ë¥¼ ì¡°ì ˆ")
    
    if time_failures:
        print(f"[{job_id}]   â° ì‹œê°„ ë¶ˆì¼ì¹˜: {len(time_failures)}ê°œ ë‹¨ì–´")
        for failure in time_failures:
            word = failure['word']
            ratio = failure['overlap_ratio']
            print(f"[{job_id}]     â””â”€ \"{word}\": {ratio*100:.1f}% ê²¹ì¹¨ (ì„ê³„ê°’ 40% ë¯¸ë‹¬)")
        print(f"[{job_id}]     â””â”€ ì œì•ˆ: ê¸°ì¤€ ìŒì„±ê³¼ ë¹„ìŠ·í•œ ì†ë„ë¡œ ë°œìŒ")
    
    if mfcc_low_quality:
        high_quality_words = [item for item in mfcc_low_quality if item['similarity'] >= 0.2]
        very_low_words = [item for item in mfcc_low_quality if item['similarity'] < 0.2]
        
        if high_quality_words:
            print(f"[{job_id}]   ğŸµ MFCC í’ˆì§ˆ ê°œì„  í•„ìš”: {len(high_quality_words)}ê°œ ë‹¨ì–´")
            print(f"[{job_id}]     â””â”€ ì œì•ˆ: ë°œìŒ ì •í™•ë„ í–¥ìƒ í•„ìš”")
        
        if very_low_words:
            print(f"[{job_id}]   ğŸµ MFCC í’ˆì§ˆ ë§¤ìš° ë‚®ìŒ: {len(very_low_words)}ê°œ ë‹¨ì–´")
            print(f"[{job_id}]     â””â”€ ì œì•ˆ: í•´ë‹¹ ë‹¨ì–´ë“¤ì˜ ë°œìŒì„ ë‹¤ì‹œ ì—°ìŠµ")
    
    if not stt_failures and not time_failures and not mfcc_low_quality:
        print(f"[{job_id}]   âœ… ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ë°œìŒ í’ˆì§ˆ")
        print(f"[{job_id}]     â””â”€ ì œì•ˆ: í˜„ì¬ ìˆ˜ì¤€ ìœ ì§€")
    
    # ìµœì¢… ê²°ê³¼ ë¡œê·¸
    print(f"[{job_id}] ğŸ“‹ ìµœì¢… ê²°ê³¼:")
    print(f"[{job_id}]   ğŸ† ì „ì²´ ì ìˆ˜: {overall_score:.3f} ({overall_score*100:.1f}%)")
    print(f"[{job_id}]   ğŸ“ í…ìŠ¤íŠ¸ ì •í™•ë„: {text_accuracy:.3f} ({text_pass_count}/{len(reference_words)})")
    print(f"[{job_id}]   â° ì‹œê°„ ì •í™•ë„: {time_accuracy:.3f} ({time_pass_count}/{len(reference_words)})")
    print(f"[{job_id}]   ğŸµ MFCC í‰ê· : {mfcc_average:.3f}")
    
    return {
        "overall_score": round(overall_score, 3),
        "word_analysis": word_analysis,
        "summary": {
            "text_accuracy": round(text_accuracy, 3),
            "time_accuracy": round(time_accuracy, 3),
            "mfcc_average": round(mfcc_average, 3),
            "total_words": len(reference_words),
            "passed_words": text_pass_count,
            "time_matched_words": time_pass_count
        },
        "failure_analysis": {
            "stt_failures": stt_failures,
            "time_failures": [f["word"] for f in time_failures],
            "mfcc_low_quality": [f["word"] for f in mfcc_low_quality]
        }
    }

# SQS ê´€ë ¨ í•¨ìˆ˜ë“¤
def sqs_message_processor():
    """SQSì—ì„œ ë©”ì‹œì§€ë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    while True:
        try:
            response = sqs_client.receive_message(
                QueueUrl=SQS_QUEUE_URL,
                MaxNumberOfMessages=1,
                WaitTimeSeconds=20
            )

            messages = response.get('Messages', [])
            for message in messages:
                print(f"ğŸ“¨ SQS ë©”ì‹œì§€ ìˆ˜ì‹ : {message['MessageId']}")

                # ë©”ì‹œì§€ ì²˜ë¦¬
                process_sqs_message(message)

                # ë©”ì‹œì§€ ì‚­ì œ
                sqs_client.delete_message(
                    QueueUrl=SQS_QUEUE_URL,
                    ReceiptHandle=message['ReceiptHandle']
                )
                print(f"âœ… SQS ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ: {message['MessageId']}")

        except Exception as e:
            print(f"âŒ SQS ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            time.sleep(5)

def process_sqs_message(message):
    """SQS ë©”ì‹œì§€ë¥¼ íŒŒì‹±í•˜ì—¬ ë¶„ì„ ì‹¤í–‰"""
    try:
        # ë©”ì‹œì§€ íŒŒì‹±
        body = json.loads(message['Body'])

        job_id = body['job_id']
        s3_audio_url = body['s3_audio_url']
        webhook_url = body['webhook_url']
        video_id = body.get('video_id', 'unknown')

        print(f"ğŸ¯ SQS ë¶„ì„ ì‹œì‘ - Job: {job_id}")

        # ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„° ì¤€ë¹„
        #script_data = get_script_data_by_video_id(video_id)

        # ê¸°ì¡´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        start_time = time.time()
        audio_path = download_from_s3(s3_audio_url, job_id)

        # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(
            analyze_pronunciation_pipeline(
                job_id, audio_path, script_data, s3_audio_url, webhook_url, start_time
            )
        )
        loop.close()

    except Exception as e:
        print(f"âŒ SQS ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œì—ë„ ì›¹í›… ì „ì†¡
        try:
            requests.post(webhook_url, json={
                "job_id": body.get('job_id', 'unknown'),
                "status": "failed",
                "error": str(e)
            }, timeout=10)
        except Exception as webhook_error:
            print(f"âŒ ì‹¤íŒ¨ ì›¹í›… ì „ì†¡ ì‹¤íŒ¨: {webhook_error}")

# def get_script_data_by_video_id(video_id: str) -> dict:
#     """video_idë¡œ ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„° ì¡°íšŒ"""
#     # TODO: ì‹¤ì œ ë°ì´í„°ë¡œ êµì²´ í•„ìš”
#     return {
#         "words": [
#             {"word": "hello", "start": 0.0, "end": 0.5, "mfcc": None},
#             {"word": "world", "start": 0.6, "end": 1.0, "mfcc": None},
#             {"word": "test", "start": 1.1, "end": 1.5, "mfcc": None}
#         ]
#     }

@app.on_event("startup")
async def startup_event():
    """FastAPI ì„œë²„ ì‹œì‘ ì‹œ SQS ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘"""
    print("ğŸš€ Voice Analysis API ì„œë²„ ì‹œì‘ - SQS ì²˜ë¦¬ ìŠ¤ë ˆë“œ í™œì„±í™”")

    sqs_thread = threading.Thread(target=sqs_message_processor, daemon=True)
    sqs_thread.start()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("pronunciation:app", host="0.0.0.0", port=8001, reload=False)